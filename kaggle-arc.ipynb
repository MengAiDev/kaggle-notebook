{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21417f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T23:28:40.009837Z",
     "iopub.status.busy": "2025-07-30T23:28:40.009514Z",
     "iopub.status.idle": "2025-07-30T23:36:50.493704Z",
     "shell.execute_reply": "2025-07-30T23:36:50.492292Z"
    },
    "papermill": {
     "duration": 490.491383,
     "end_time": "2025-07-30T23:36:50.495661",
     "exception": false,
     "start_time": "2025-07-30T23:28:40.004278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Kaggle dataset folder: /kaggle/input/arc-prize-2025\n",
      "Using data directory: /kaggle/input/arc-prize-2025\n",
      "Files in directory: ['arc-agi_training_solutions.json', 'arc-agi_evaluation_solutions.json', 'arc-agi_evaluation_challenges.json', 'sample_submission.json', 'arc-agi_training_challenges.json', 'arc-agi_test_challenges.json']\n",
      "Model file neuro_symbolic_model.pth not found\n",
      "Loading training data for model training...\n",
      "Training neural-symbolic model...\n",
      "Dataset initialized with 3232 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 202/202 [00:18<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0158 (Rule: 0.0206, Param: 0.0045)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 202/202 [00:17<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.0001 (Rule: 0.0000, Param: 0.0002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 202/202 [00:18<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 202/202 [00:23<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 202/202 [00:23<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 202/202 [00:23<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.0005 (Rule: 0.0000, Param: 0.0015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 202/202 [00:22<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 202/202 [00:22<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 202/202 [00:22<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 202/202 [00:23<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 202/202 [00:22<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 202/202 [00:23<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 202/202 [00:23<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 202/202 [00:23<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 202/202 [00:23<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 202/202 [00:22<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 202/202 [00:23<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 202/202 [00:23<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 202/202 [00:23<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 202/202 [00:22<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.0000 (Rule: 0.0000, Param: 0.0000)\n",
      "Model saved to 'neuro_symbolic_model.pth'\n",
      "Loading test challenges...\n",
      "Solving ARC tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tasks: 100%|██████████| 240/240 [00:26<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission to submission.json\n",
      "Done! Submission file saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import label\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 设置随机种子确保可复现性\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# --- 数据加载策略 ---\n",
    "def find_data_dir():\n",
    "    \"\"\"自动检测Kaggle数据目录或本地数据目录\"\"\"\n",
    "    kaggle_base = '/kaggle/input'\n",
    "    expected_files = {\n",
    "        'arc-agi_evaluation_challenges.json',\n",
    "        'arc-agi_evaluation_solutions.json',\n",
    "        'arc-agi_training_solutions.json'\n",
    "    }\n",
    "    if os.path.exists(kaggle_base):\n",
    "        for folder in os.listdir(kaggle_base):\n",
    "            full_path = os.path.join(kaggle_base, folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                files = set(os.listdir(full_path))\n",
    "                if expected_files.issubset(files):\n",
    "                    print(f\"Found Kaggle dataset folder: {full_path}\")\n",
    "                    return full_path\n",
    "    print(\"Kaggle dataset folder not found, falling back to './data'\")\n",
    "    return './data'\n",
    "\n",
    "def load_arc_data(challenge_file: str, solution_file: Optional[str] = None) -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"加载ARC挑战数据和解决方案\"\"\"\n",
    "    with open(challenge_file, 'r') as f:\n",
    "        challenges = json.load(f)\n",
    "    solutions = None\n",
    "    if solution_file and os.path.exists(solution_file):\n",
    "        with open(solution_file, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "    return challenges, solutions\n",
    "\n",
    "# --- 网格处理工具 ---\n",
    "def grid_to_numpy(grid: List[List[int]]) -> np.ndarray:\n",
    "    \"\"\"将网格转换为NumPy数组\"\"\"\n",
    "    return np.array(grid, dtype=np.int8)\n",
    "\n",
    "def numpy_to_grid(array: np.ndarray) -> List[List[int]]:\n",
    "    \"\"\"将NumPy数组转换回网格格式\"\"\"\n",
    "    return array.tolist()\n",
    "\n",
    "def resize_grid(grid: np.ndarray, new_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"调整网格大小\"\"\"\n",
    "    old_h, old_w = grid.shape\n",
    "    new_h, new_w = new_shape\n",
    "    row_ratio = old_h / new_h\n",
    "    col_ratio = old_w / new_w\n",
    "    resized = np.zeros((new_h, new_w), dtype=grid.dtype)\n",
    "    for r in range(new_h):\n",
    "        for c in range(new_w):\n",
    "            src_r = min(int(r * row_ratio), old_h - 1)\n",
    "            src_c = min(int(c * col_ratio), old_w - 1)\n",
    "            resized[r, c] = grid[src_r, src_c]\n",
    "    return resized\n",
    "\n",
    "def pad_grid(grid: List[List[int]], max_size: int = 30) -> List[List[int]]:\n",
    "    \"\"\"填充网格到标准大小\"\"\"\n",
    "    grid = grid_to_numpy(grid)\n",
    "    h, w = grid.shape\n",
    "    # 计算填充量\n",
    "    pad_h = max(0, max_size - h)\n",
    "    pad_w = max(0, max_size - w)\n",
    "    # 在右侧和底部添加填充\n",
    "    padded = np.pad(grid, ((0, pad_h), (0, pad_w)), \n",
    "                   mode='constant', constant_values=0)\n",
    "    return padded.tolist()\n",
    "\n",
    "# --- 特征提取和相似度计算 ---\n",
    "def extract_features(grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"提取网格特征\"\"\"\n",
    "    features = []\n",
    "    h, w = grid.shape\n",
    "    features.extend([h, w, h*w])\n",
    "    # 颜色统计特征\n",
    "    unique_colors, counts = np.unique(grid, return_counts=True)\n",
    "    features.append(len(unique_colors))\n",
    "    # 颜色直方图\n",
    "    hist = np.zeros(10)\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        if 0 <= color < 10:\n",
    "            hist[color] = count\n",
    "    features.extend(hist / (h * w + 1e-5))\n",
    "    # 对象特征\n",
    "    num_objects = 0\n",
    "    total_object_area = 0\n",
    "    if grid.any():\n",
    "        labeled_array, num_features = label(grid > 0)\n",
    "        if num_features > 0:\n",
    "            num_objects = num_features\n",
    "            object_sizes = [np.sum(labeled_array == i) for i in range(1, num_features + 1)]\n",
    "            total_object_area = sum(object_sizes)\n",
    "            features.extend([np.mean(object_sizes), np.std(object_sizes), max(object_sizes)])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])\n",
    "    else:\n",
    "        features.extend([0, 0, 0])\n",
    "    features.append(num_objects)\n",
    "    features.append(total_object_area / (h * w + 1e-5))\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def combined_similarity(g1: np.ndarray, g2: np.ndarray) -> float:\n",
    "    \"\"\"计算网格综合相似度\"\"\"\n",
    "    feat1 = extract_features(g1)\n",
    "    feat2 = extract_features(g2)\n",
    "    # 特征嵌入相似度\n",
    "    sim_embed = cosine_similarity(feat1.reshape(1, -1), feat2.reshape(1, -1))[0, 0]\n",
    "    # 颜色直方图相似度\n",
    "    max_color = max(g1.max(), g2.max(), 9)\n",
    "    hist1, _ = np.histogram(g1, bins=np.arange(max_color+2))\n",
    "    hist2, _ = np.histogram(g2, bins=np.arange(max_color+2))\n",
    "    hist1 = hist1 / (hist1.sum() + 1e-5)\n",
    "    hist2 = hist2 / (hist2.sum() + 1e-5)\n",
    "    sim_hist = np.dot(hist1, hist2) / (np.linalg.norm(hist1)*np.linalg.norm(hist2) + 1e-5)\n",
    "    return 0.7 * sim_embed + 0.3 * sim_hist\n",
    "\n",
    "# --- 变换操作 ---\n",
    "def rotate(grid: np.ndarray, k: int = 1) -> np.ndarray:\n",
    "    \"\"\"旋转网格\"\"\"\n",
    "    return np.rot90(grid, k=k)\n",
    "\n",
    "def flip(grid: np.ndarray, direction: str = 'h') -> np.ndarray:\n",
    "    \"\"\"翻转网格\"\"\"\n",
    "    return np.fliplr(grid) if direction == 'h' else np.flipud(grid)\n",
    "\n",
    "def translate(grid: np.ndarray, dx: int = 0, dy: int = 0) -> np.ndarray:\n",
    "    \"\"\"平移网格\"\"\"\n",
    "    shifted = np.zeros_like(grid)\n",
    "    h, w = grid.shape\n",
    "    # 计算源区域和目标区域\n",
    "    x_start_src = max(0, -dx)\n",
    "    x_end_src = w - max(0, dx)\n",
    "    y_start_src = max(0, -dy)\n",
    "    y_end_src = h - max(0, dy)\n",
    "    x_start_dst = max(0, dx)\n",
    "    x_end_dst = w - max(0, -dx)\n",
    "    y_start_dst = max(0, dy)\n",
    "    y_end_dst = h - max(0, -dy)\n",
    "    # 执行平移\n",
    "    shifted[y_start_dst:y_end_dst, x_start_dst:x_end_dst] = grid[y_start_src:y_end_src, x_start_src:x_end_src]\n",
    "    return shifted\n",
    "\n",
    "def scale(grid: np.ndarray, factor: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"缩放网格\"\"\"\n",
    "    if factor == 1.0:\n",
    "        return grid.copy()\n",
    "    h, w = grid.shape\n",
    "    new_h = max(1, int(h * factor))\n",
    "    new_w = max(1, int(w * factor))\n",
    "    return resize_grid(grid, (new_h, new_w))\n",
    "\n",
    "# --- 神经符号模型 ---\n",
    "class NeuroSymbolicModel(nn.Module):\n",
    "    \"\"\"神经符号混合模型\"\"\"\n",
    "    def __init__(self, max_grid_size: int = 30, max_colors: int = 10, num_primitives: int = 20):\n",
    "        super().__init__()\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.max_colors = max_colors\n",
    "        self.num_primitives = num_primitives\n",
    "        # 卷积特征提取器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(max_colors, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        # 规则预测头\n",
    "        self.rule_head = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_primitives)\n",
    "        )\n",
    "        # 参数预测头\n",
    "        self.param_head = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)  # 最多4个参数\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        features = self.encoder(x)\n",
    "        features_flat = features.view(features.size(0), -1)\n",
    "        rule_logits = self.rule_head(features_flat)\n",
    "        params = self.param_head(features_flat)\n",
    "        return rule_logits, params\n",
    "    \n",
    "    def grid_to_tensor(self, grid: List[List[int]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        将网格转换为模型输入张量\n",
    "        返回形状: [max_colors, max_grid_size, max_grid_size]\n",
    "        \"\"\"\n",
    "        grid = grid_to_numpy(grid)\n",
    "        tensor = np.zeros((self.max_colors, self.max_grid_size, self.max_grid_size))\n",
    "        for c in range(self.max_colors):\n",
    "            tensor[c] = (grid == c).astype(np.float32)\n",
    "        return torch.tensor(tensor, dtype=torch.float32)\n",
    "\n",
    "# --- 自定义数据集 ---\n",
    "class ARCTrainDataset(Dataset):\n",
    "    \"\"\"ARC训练数据集\"\"\"\n",
    "    def __init__(self, training_data: Dict, max_grid_size: int = 30):\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.samples = []\n",
    "        \n",
    "        # 支持的变换类型\n",
    "        self.primitive_names = [\n",
    "            'rotate90', 'rotate180', 'rotate270', \n",
    "            'flip_h', 'flip_v', 'color_shift',\n",
    "            'identity'\n",
    "        ]\n",
    "        \n",
    "        # 收集训练样本\n",
    "        for task_id, task in training_data.items():\n",
    "            for pair in task['train']:\n",
    "                input_grid = pad_grid(pair['input'], self.max_grid_size)\n",
    "                output_grid = pad_grid(pair['output'], self.max_grid_size)\n",
    "                \n",
    "                # 识别变换\n",
    "                input_np = grid_to_numpy(input_grid)\n",
    "                output_np = grid_to_numpy(output_grid)\n",
    "                transform_type, params = self.identify_transformation(input_np, output_np)\n",
    "                \n",
    "                # 只保留我们支持的变换类型\n",
    "                if transform_type in self.primitive_names:\n",
    "                    self.samples.append({\n",
    "                        'input': input_grid,\n",
    "                        'transform_type': transform_type,\n",
    "                        'params': params\n",
    "                    })\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_tensor = self.grid_to_tensor(sample['input'])\n",
    "        \n",
    "        # 规则目标\n",
    "        rule_idx = self.primitive_names.index(sample['transform_type'])\n",
    "        \n",
    "        # 参数目标 (填充到固定长度)\n",
    "        params = sample['params'] + [0.0] * (4 - len(sample['params']))\n",
    "        \n",
    "        return input_tensor, rule_idx, np.array(params, dtype=np.float32)\n",
    "    \n",
    "    def grid_to_tensor(self, grid: List[List[int]]) -> torch.Tensor:\n",
    "        \"\"\"将网格转换为模型输入张量\"\"\"\n",
    "        grid = grid_to_numpy(grid)\n",
    "        tensor = np.zeros((10, self.max_grid_size, self.max_grid_size))\n",
    "        for c in range(10):\n",
    "            tensor[c] = (grid == c).astype(np.float32)\n",
    "        return torch.tensor(tensor, dtype=torch.float32)\n",
    "    \n",
    "    def identify_transformation(self, input_grid: np.ndarray, output_grid: np.ndarray) -> Tuple[str, List[float]]:\n",
    "        \"\"\"\n",
    "        识别从输入网格到输出网格的变换\n",
    "        \n",
    "        返回: (变换类型, 参数列表)\n",
    "        \"\"\"\n",
    "        # 检查是否相同 (identity)\n",
    "        if np.array_equal(input_grid, output_grid):\n",
    "            return 'identity', []\n",
    "        \n",
    "        # 检查旋转\n",
    "        if np.array_equal(output_grid, rotate(input_grid, 1)):\n",
    "            return 'rotate90', []\n",
    "        if np.array_equal(output_grid, rotate(input_grid, 2)):\n",
    "            return 'rotate180', []\n",
    "        if np.array_equal(output_grid, rotate(input_grid, 3)):\n",
    "            return 'rotate270', []\n",
    "        \n",
    "        # 检查水平翻转\n",
    "        if np.array_equal(output_grid, flip(input_grid, 'h')):\n",
    "            return 'flip_h', []\n",
    "        \n",
    "        # 检查垂直翻转\n",
    "        if np.array_equal(output_grid, flip(input_grid, 'v')):\n",
    "            return 'flip_v', []\n",
    "        \n",
    "        # 检查颜色变换\n",
    "        unique_in = np.unique(input_grid)\n",
    "        unique_out = np.unique(output_grid)\n",
    "        \n",
    "        # 检查是否为简单的颜色偏移\n",
    "        if len(unique_in) == len(unique_out) and len(unique_in) > 0:\n",
    "            # 计算可能的颜色映射\n",
    "            color_map = {}\n",
    "            consistent = True\n",
    "            \n",
    "            for i in range(len(unique_in)):\n",
    "                diff = unique_out[i] - unique_in[i]\n",
    "                if i > 0 and diff != color_map.get('diff', diff):\n",
    "                    consistent = False\n",
    "                    break\n",
    "                color_map[unique_in[i]] = unique_out[i]\n",
    "                color_map['diff'] = diff\n",
    "            \n",
    "            if consistent:\n",
    "                # 验证映射是否一致\n",
    "                transformed = np.zeros_like(input_grid)\n",
    "                for r in range(input_grid.shape[0]):\n",
    "                    for c in range(input_grid.shape[1]):\n",
    "                        transformed[r, c] = color_map.get(input_grid[r, c], input_grid[r, c])\n",
    "                \n",
    "                if np.array_equal(transformed, output_grid):\n",
    "                    return 'color_shift', [color_map['diff']]\n",
    "        \n",
    "        # 默认返回identity (虽然不完全匹配，但比unknown好)\n",
    "        return 'identity', []\n",
    "\n",
    "# --- ARC求解器 ---\n",
    "class HybridARCSolver:\n",
    "    \"\"\"混合ARC求解器（神经符号+变换匹配）\"\"\"\n",
    "    def __init__(self, max_grid_size: int = 30):\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.primitive_names = [\n",
    "            'rotate90', 'rotate180', 'rotate270', \n",
    "            'flip_h', 'flip_v', 'color_shift',\n",
    "            'identity'\n",
    "        ]\n",
    "        self.model = NeuroSymbolicModel(\n",
    "            max_grid_size, \n",
    "            num_primitives=len(self.primitive_names)\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # 初始化时设为评估模式\n",
    "    \n",
    "    def train(self, training_data: Dict, num_epochs: int = 20, batch_size: int = 16, lr: float = 1e-3):\n",
    "        \"\"\"训练神经符号模型\"\"\"\n",
    "        # 创建数据集和数据加载器\n",
    "        dataset = ARCTrainDataset(training_data, self.max_grid_size)\n",
    "        \n",
    "        if len(dataset) == 0:\n",
    "            print(\"Warning: No valid training samples found. Using default model.\")\n",
    "            return\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 设置模型为训练模式\n",
    "        self.model.train()\n",
    "        \n",
    "        # 优化器和损失函数\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        rule_criterion = nn.CrossEntropyLoss()\n",
    "        param_criterion = nn.MSELoss()\n",
    "        \n",
    "        # 训练循环\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            total_rule_loss = 0\n",
    "            total_param_loss = 0\n",
    "            \n",
    "            for inputs, rule_targets, param_targets in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                # 将数据移动到设备\n",
    "                inputs = inputs.to(self.device)\n",
    "                rule_targets = rule_targets.to(self.device)\n",
    "                param_targets = param_targets.to(self.device)\n",
    "                \n",
    "                # 前向传播\n",
    "                rule_logits, predicted_params = self.model(inputs)\n",
    "                \n",
    "                # 计算损失\n",
    "                rule_loss = rule_criterion(rule_logits, rule_targets)\n",
    "                param_loss = param_criterion(predicted_params, param_targets)\n",
    "                \n",
    "                # 总损失 (加权组合)\n",
    "                loss = 0.7 * rule_loss + 0.3 * param_loss\n",
    "                \n",
    "                # 反向传播\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 记录损失\n",
    "                total_loss += loss.item()\n",
    "                total_rule_loss += rule_loss.item()\n",
    "                total_param_loss += param_loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            avg_rule_loss = total_rule_loss / len(dataloader)\n",
    "            avg_param_loss = total_param_loss / len(dataloader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f} \"\n",
    "                  f\"(Rule: {avg_rule_loss:.4f}, Param: {avg_param_loss:.4f})\")\n",
    "        \n",
    "        # 保存训练好的模型\n",
    "        torch.save(self.model.state_dict(), 'neuro_symbolic_model.pth')\n",
    "        print(\"Model saved to 'neuro_symbolic_model.pth'\")\n",
    "        self.model.eval()  # 训练完成后设回评估模式\n",
    "    \n",
    "    def load_model(self, model_path: str = 'neuro_symbolic_model.pth'):\n",
    "        \"\"\"加载训练好的模型\"\"\"\n",
    "        if os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "            self.model.eval()\n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Model file {model_path} not found\")\n",
    "            return False\n",
    "    \n",
    "    def solve_task(self, task: Dict[str, Any]) -> List[List[List[int]]]:\n",
    "        \"\"\"解决单个ARC任务\"\"\"\n",
    "        # 预处理训练对\n",
    "        train_pairs = []\n",
    "        for pair in task['train']:\n",
    "            input_grid = pad_grid(pair['input'], self.max_grid_size)\n",
    "            output_grid = pad_grid(pair['output'], self.max_grid_size)\n",
    "            train_pairs.append({\n",
    "                'input': input_grid,\n",
    "                'output': output_grid\n",
    "            })\n",
    "        \n",
    "        # 处理测试输入\n",
    "        test_inputs = []\n",
    "        for test in task['test']:\n",
    "            input_grid = pad_grid(test['input'], self.max_grid_size)\n",
    "            test_inputs.append({\n",
    "                'input': input_grid\n",
    "            })\n",
    "        \n",
    "        # 生成预测\n",
    "        predictions = []\n",
    "        for test in test_inputs:\n",
    "            input_grid = test['input']\n",
    "            # 尝试1：神经符号方法\n",
    "            try:\n",
    "                attempt1 = self.neuro_symbolic_solution(input_grid, train_pairs)\n",
    "            except Exception as e:\n",
    "                print(f\"Neuro-symbolic solution failed: {str(e)}\")\n",
    "                attempt1 = self.transform_based_solution(input_grid, train_pairs)\n",
    "            \n",
    "            # 尝试2：变换匹配方法\n",
    "            attempt2 = self.transform_based_solution(input_grid, train_pairs)\n",
    "            \n",
    "            # 选择最佳结果\n",
    "            best_attempt = self.select_best_attempt(input_grid, attempt1, attempt2, train_pairs)\n",
    "            predictions.append(best_attempt)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def select_best_attempt(self, input_grid: List[List[int]], \n",
    "                           attempt1: List[List[int]], attempt2: List[List[int]],\n",
    "                           train_pairs: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"选择最佳预测结果\"\"\"\n",
    "        input_np = grid_to_numpy(input_grid)\n",
    "        attempt1_np = grid_to_numpy(attempt1)\n",
    "        attempt2_np = grid_to_numpy(attempt2)\n",
    "        \n",
    "        # 评估每个尝试与训练对的匹配度\n",
    "        score1 = self.evaluate_attempt(attempt1_np, train_pairs)\n",
    "        score2 = self.evaluate_attempt(attempt2_np, train_pairs)\n",
    "        \n",
    "        # 选择得分更高的尝试\n",
    "        if score1 >= score2:\n",
    "            return attempt1\n",
    "        else:\n",
    "            return attempt2\n",
    "    \n",
    "    def evaluate_attempt(self, output_grid: np.ndarray, \n",
    "                        train_pairs: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"评估输出网格与训练对的匹配度\"\"\"\n",
    "        total_score = 0\n",
    "        count = 0\n",
    "        \n",
    "        for train in train_pairs:\n",
    "            train_out = grid_to_numpy(train['output'])\n",
    "            # 计算与训练输出的相似度\n",
    "            sim = combined_similarity(output_grid, train_out)\n",
    "            total_score += sim\n",
    "            count += 1\n",
    "        \n",
    "        return total_score / count if count > 0 else 0\n",
    "    \n",
    "    def neuro_symbolic_solution(self, input_grid: List[List[int]], \n",
    "                              train_pairs: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"神经符号解决方案\"\"\"\n",
    "        try:\n",
    "            # 将网格转换为张量 [channels, height, width]\n",
    "            tensor = self.model.grid_to_tensor(input_grid)\n",
    "            # 添加batch维度 [1, channels, height, width]\n",
    "            tensor = tensor.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # 使用模型预测\n",
    "            with torch.no_grad():\n",
    "                rule_logits, params = self.model(tensor)\n",
    "            \n",
    "            # 获取最高置信度的规则\n",
    "            rule_probs = torch.softmax(rule_logits, dim=1)\n",
    "            top_rule_idx = torch.argmax(rule_probs).item()\n",
    "            rule_name = self.primitive_names[top_rule_idx]\n",
    "            \n",
    "            # 应用规则\n",
    "            grid_np = grid_to_numpy(input_grid)\n",
    "            if rule_name == 'rotate90':\n",
    "                result = rotate(grid_np, 1)\n",
    "            elif rule_name == 'rotate180':\n",
    "                result = rotate(grid_np, 2)\n",
    "            elif rule_name == 'rotate270':\n",
    "                result = rotate(grid_np, 3)\n",
    "            elif rule_name == 'flip_h':\n",
    "                result = flip(grid_np, 'h')\n",
    "            elif rule_name == 'flip_v':\n",
    "                result = flip(grid_np, 'v')\n",
    "            elif rule_name == 'color_shift':\n",
    "                shift = int(params[0][0].item() % 10)\n",
    "                if shift == 0:\n",
    "                    shift = 1  # 避免除零\n",
    "                result = (grid_np + shift) % 10\n",
    "            else:  # identity\n",
    "                result = grid_np\n",
    "            \n",
    "            # 移除填充\n",
    "            return self.remove_padding(result.tolist(), train_pairs)\n",
    "        except Exception as e:\n",
    "            print(f\"Neuro-symbolic solution failed: {str(e)}\")\n",
    "            # 如果神经符号方法失败，使用简单回退\n",
    "            return self.transform_based_solution(input_grid, train_pairs)\n",
    "    \n",
    "    def transform_based_solution(self, input_grid: List[List[int]], \n",
    "                                train_pairs: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"基于变换的解决方案\"\"\"\n",
    "        test_in = grid_to_numpy(input_grid)\n",
    "        candidates = []\n",
    "        \n",
    "        # 生成候选变换\n",
    "        for flip_dir in ['none', 'h', 'v']:\n",
    "            base = test_in.copy()\n",
    "            if flip_dir != 'none':\n",
    "                base = flip(base, flip_dir)\n",
    "            for rot in range(4):\n",
    "                r = rotate(base, rot)\n",
    "                candidates.append(r)\n",
    "        \n",
    "        # 评估候选变换\n",
    "        best_sim = -1\n",
    "        best_output = None\n",
    "        for cand in candidates:\n",
    "            for train in train_pairs:\n",
    "                train_in = grid_to_numpy(train['input'])\n",
    "                train_out = grid_to_numpy(train['output'])\n",
    "                sim_in = combined_similarity(cand, train_in)\n",
    "                sim_out = combined_similarity(cand, train_out)\n",
    "                sim = max(sim_in, sim_out)\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_output = train_out\n",
    "        \n",
    "        # 返回最佳匹配输出\n",
    "        if best_output is not None:\n",
    "            return self.remove_padding(best_output.tolist(), train_pairs)\n",
    "        \n",
    "        # 如果没有找到匹配，尝试更复杂的分析\n",
    "        return self.pattern_based_solution(input_grid, train_pairs)\n",
    "    \n",
    "    def pattern_based_solution(self, input_grid: List[List[int]], \n",
    "                              train_pairs: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"基于模式的解决方案（更复杂的逻辑）\"\"\"\n",
    "        # 这里可以添加更高级的模式识别和规则归纳\n",
    "        # 例如：对象识别、关系推理等\n",
    "        \n",
    "        # 作为简单示例，返回输入网格\n",
    "        return input_grid\n",
    "    \n",
    "    def remove_padding(self, grid: List[List[int]], train_pairs: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"移除填充，恢复原始网格大小\"\"\"\n",
    "        # 尝试从训练对中确定原始尺寸\n",
    "        original_height, original_width = None, None\n",
    "        \n",
    "        for pair in train_pairs:\n",
    "            orig_in = pair['input']\n",
    "            orig_out = pair['output']\n",
    "            \n",
    "            # 使用输出尺寸作为参考\n",
    "            if original_height is None or original_width is None:\n",
    "                original_height, original_width = len(orig_out), len(orig_out[0])\n",
    "        \n",
    "        # 如果无法确定原始尺寸，使用输入尺寸\n",
    "        if original_height is None or original_width is None:\n",
    "            original_height, original_width = len(input_grid), len(input_grid[0])\n",
    "        \n",
    "        # 裁剪到原始尺寸\n",
    "        grid_np = grid_to_numpy(grid)\n",
    "        return grid_np[:original_height, :original_width].tolist()\n",
    "\n",
    "# --- 主程序 ---\n",
    "def main():\n",
    "    # 自动检测数据目录\n",
    "    DATA_DIR = find_data_dir()\n",
    "    print(f\"Using data directory: {DATA_DIR}\")\n",
    "    print(\"Files in directory:\", os.listdir(DATA_DIR))\n",
    "    \n",
    "    # 构建文件路径\n",
    "    TRAIN_CHALLENGES = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\n",
    "    TRAIN_SOLUTIONS = os.path.join(DATA_DIR, 'arc-agi_training_solutions.json')\n",
    "    EVAL_CHALLENGES = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\n",
    "    TEST_CHALLENGES = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\n",
    "    \n",
    "    # 初始化求解器\n",
    "    solver = HybridARCSolver()\n",
    "    \n",
    "    # 尝试加载预训练模型\n",
    "    if solver.load_model():\n",
    "        print(\"Using pre-trained model\")\n",
    "    else:\n",
    "        # 如果模型不存在，检查是否有训练数据\n",
    "        if os.path.exists(TRAIN_CHALLENGES) and os.path.exists(TRAIN_SOLUTIONS):\n",
    "            print(\"Loading training data for model training...\")\n",
    "            with open(TRAIN_CHALLENGES, 'r') as f:\n",
    "                training_challenges = json.load(f)\n",
    "            \n",
    "            # 训练模型\n",
    "            print(\"Training neural-symbolic model...\")\n",
    "            solver.train(training_challenges)\n",
    "        else:\n",
    "            print(\"Training data not found. Using default model.\")\n",
    "    \n",
    "    # 加载测试挑战数据\n",
    "    print(\"Loading test challenges...\")\n",
    "    with open(TEST_CHALLENGES, 'r') as f:\n",
    "        test_challenges = json.load(f)\n",
    "    \n",
    "    # 生成提交\n",
    "    submission = {}\n",
    "    print(\"Solving ARC tasks...\")\n",
    "    for task_id, task_data in tqdm(test_challenges.items(), desc=\"Processing tasks\"):\n",
    "        predictions = solver.solve_task(task_data)\n",
    "        # ARC要求的提交格式：每个任务一个列表，每个测试输入一个输出网格\n",
    "        submission[task_id] = predictions\n",
    "    \n",
    "    # 保存提交文件\n",
    "    output_path = 'submission.json'\n",
    "    print(f\"Saving submission to {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(submission, f)\n",
    "    print(\"Done! Submission file saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 499.460539,
   "end_time": "2025-07-30T23:36:53.416684",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-30T23:28:33.956145",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
