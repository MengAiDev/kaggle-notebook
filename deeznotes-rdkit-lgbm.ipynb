{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee831040",
   "metadata": {
    "_cell_guid": "f7a858b5-fd7f-45f9-9440-968ea2b0cf23",
    "_uuid": "0ab1f154-55e8-494d-9190-2eac756fdb47",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:05:40.512618Z",
     "iopub.status.busy": "2025-08-07T09:05:40.511583Z",
     "iopub.status.idle": "2025-08-07T09:05:42.386774Z",
     "shell.execute_reply": "2025-08-07T09:05:42.385880Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.883998,
     "end_time": "2025-08-07T09:05:42.388712",
     "exception": false,
     "start_time": "2025-08-07T09:05:40.504714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tc-smiles/Tc_SMILES.csv\n",
      "/kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\n",
      "/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\n",
      "/kaggle/input/smiles-extra-data/data_dnst1.xlsx\n",
      "/kaggle/input/smiles-extra-data/data_tg3.xlsx\n",
      "/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\n",
      "/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a70fbf",
   "metadata": {
    "_cell_guid": "681f2f3a-3f78-4618-826b-c13154181ba0",
    "_uuid": "0c0f5f1f-4bfe-4841-8315-eedaa3b8129d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:05:42.398752Z",
     "iopub.status.busy": "2025-08-07T09:05:42.398311Z",
     "iopub.status.idle": "2025-08-07T09:05:49.489998Z",
     "shell.execute_reply": "2025-08-07T09:05:49.488482Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 7.099215,
     "end_time": "2025-08-07T09:05:49.492542",
     "exception": false,
     "start_time": "2025-08-07T09:05:42.393327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\r\n",
      "Installing collected packages: rdkit\r\n",
      "Successfully installed rdkit-2025.3.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05897bf",
   "metadata": {
    "_cell_guid": "facbdeef-a6d8-453e-8d9a-28bf492078a3",
    "_uuid": "6644218f-7a0e-41ee-8756-b760cba45de5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00379,
     "end_time": "2025-08-07T09:05:49.500934",
     "exception": false,
     "start_time": "2025-08-07T09:05:49.497144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eed6d6c",
   "metadata": {
    "_cell_guid": "c3c51df5-fa1b-4882-bcec-441033c20bae",
    "_uuid": "f7f115ac-5ca4-475c-a8ea-8b38deab049e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:05:49.510902Z",
     "iopub.status.busy": "2025-08-07T09:05:49.510541Z",
     "iopub.status.idle": "2025-08-07T09:06:00.348561Z",
     "shell.execute_reply": "2025-08-07T09:06:00.347366Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 10.845217,
     "end_time": "2025-08-07T09:06:00.350110",
     "exception": false,
     "start_time": "2025-08-07T09:05:49.504893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv (874 entries for Tc)\n",
      "✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv (46 entries for Tg)\n",
      "✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv (862 entries for FFV)\n",
      "✅ Integrated data_tg3.xlsx: 499 entries for Tg\n",
      "✅ Integrated data_dnst1.xlsx: 778 entries for Density\n",
      "✅ Integrated Tc_SMILES.csv: 866 entries for Tc\n",
      "✅ Loaded JCIM SMILES-only dataset: 662 unique SMILES (no targets)\n",
      "\n",
      "📊 Final Summary:\n",
      "Train: 7973 | Extended: 9990\n",
      "• Tg      : 1056 total (+545 from supplements)\n",
      "• FFV     : 7892 total (+862 from supplements)\n",
      "• Tc      : 866 total (+129 from supplements)\n",
      "• Density : 1247 total (+634 from supplements)\n",
      "• Rg      : 614 total (+0 from supplements)\n",
      "\n",
      "✅ Data loading and preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "# === Config ===\n",
    "BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "BAD_PATTERNS = ['[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]',\n",
    "                \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "                '([R])', '([R1])', '([R2])']\n",
    "\n",
    "# === SMILES Cleaner ===\n",
    "def clean_and_validate_smiles(smiles):\n",
    "    if not isinstance(smiles, str) or not smiles:\n",
    "        return None\n",
    "    for pattern in BAD_PATTERNS:\n",
    "        if pattern in smiles:\n",
    "            return None\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# === Load Train/Test ===\n",
    "train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "train.dropna(subset=['SMILES'], inplace=True)\n",
    "test.dropna(subset=['SMILES'], inplace=True)\n",
    "\n",
    "# === Load External Datasets (excluding dataset2) ===\n",
    "external_datasets = []\n",
    "\n",
    "def load_external(path, target, rename_map=None):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if rename_map:\n",
    "            df = df.rename(columns=rename_map)\n",
    "        if 'SMILES' in df.columns and target in df.columns:\n",
    "            df = df[['SMILES', target]].dropna()\n",
    "            external_datasets.append((target, df))\n",
    "            print(f\"✅ Loaded {path} ({len(df)} entries for {target})\")\n",
    "        else:\n",
    "            print(f\"⚠️ Skipped {path}: required columns missing\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {path}: {e}\")\n",
    "\n",
    "load_external(BASE_PATH + 'train_supplement/dataset1.csv', 'Tc', rename_map={'TC_mean': 'Tc'})\n",
    "load_external(BASE_PATH + 'train_supplement/dataset3.csv', 'Tg')\n",
    "load_external(BASE_PATH + 'train_supplement/dataset4.csv', 'FFV')\n",
    "\n",
    "# === Load Additional External Datasets ===\n",
    "try:\n",
    "    extra_data_tg3 = pd.read_excel(\"/kaggle/input/smiles-extra-data/data_tg3.xlsx\")\n",
    "    extra_data_dnst1 = pd.read_excel(\"/kaggle/input/smiles-extra-data/data_dnst1.xlsx\")\n",
    "    jcim_sup_bigsmiles = pd.read_csv(\"/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\")\n",
    "    tc_smiles_df = pd.read_csv(\"/kaggle/input/tc-smiles/Tc_SMILES.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error loading extra data: {e}\")\n",
    "\n",
    "# Helper to standardize and append\n",
    "def process_and_append_external(df, target, source_name):\n",
    "    if 'SMILES' in df.columns and target in df.columns:\n",
    "        df = df[['SMILES', target]].copy()\n",
    "        df['SMILES'] = df['SMILES'].apply(clean_and_validate_smiles)\n",
    "        df = df.dropna(subset=['SMILES'])\n",
    "\n",
    "        # Ensure the target column is numeric\n",
    "        df[target] = pd.to_numeric(df[target], errors='coerce')\n",
    "        df = df.dropna(subset=[target])\n",
    "\n",
    "        df = df.groupby('SMILES', as_index=False)[target].mean()\n",
    "        external_datasets.append((target, df))\n",
    "        print(f\"✅ Integrated {source_name}: {len(df)} entries for {target}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Skipped {source_name}: missing columns\")\n",
    "\n",
    "# Process each extra dataset (with correct column names)\n",
    "process_and_append_external(extra_data_tg3.rename(columns={\"Tg [K]\": \"Tg\"}), \"Tg\", \"data_tg3.xlsx\")\n",
    "process_and_append_external(extra_data_dnst1.rename(columns={\"density(g/cm3)\": \"Density\"}), \"Density\", \"data_dnst1.xlsx\")\n",
    "process_and_append_external(tc_smiles_df.rename(columns={\"TC_mean\": \"Tc\"}), \"Tc\", \"Tc_SMILES.csv\")\n",
    "\n",
    "# JCIM SMILES only (for future feature engineering)\n",
    "jcim_smiles_only = jcim_sup_bigsmiles[['SMILES']].dropna()\n",
    "jcim_smiles_only['SMILES'] = jcim_smiles_only['SMILES'].apply(clean_and_validate_smiles)\n",
    "jcim_smiles_only = jcim_smiles_only.dropna().drop_duplicates()\n",
    "print(f\"✅ Loaded JCIM SMILES-only dataset: {len(jcim_smiles_only)} unique SMILES (no targets)\")\n",
    "\n",
    "# === Merge External Data ===\n",
    "def merge_external(train_df, ext_df, target):\n",
    "    ext_df['SMILES'] = ext_df['SMILES'].apply(clean_and_validate_smiles)\n",
    "    ext_df = ext_df.dropna(subset=['SMILES', target])\n",
    "    ext_df = ext_df.groupby('SMILES', as_index=False)[target].mean()\n",
    "\n",
    "    # Fill missing target values in existing rows\n",
    "    existing_smiles = set(train_df['SMILES'])\n",
    "    to_fill = ext_df[ext_df['SMILES'].isin(existing_smiles)]\n",
    "    for _, row in to_fill.iterrows():\n",
    "        mask = (train_df['SMILES'] == row['SMILES']) & (train_df[target].isna())\n",
    "        train_df.loc[mask, target] = row[target]\n",
    "\n",
    "    # Add new rows\n",
    "    new_smiles = set(ext_df['SMILES']) - existing_smiles\n",
    "    new_rows = ext_df[ext_df['SMILES'].isin(new_smiles)].copy()\n",
    "    for col in TARGETS:\n",
    "        if col not in new_rows.columns:\n",
    "            new_rows[col] = np.nan\n",
    "    return pd.concat([train_df, new_rows[['SMILES'] + TARGETS]], ignore_index=True)\n",
    "\n",
    "# === Apply Merges ===\n",
    "train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "for target, ext in external_datasets:\n",
    "    train_extended = merge_external(train_extended, ext, target)\n",
    "\n",
    "# === Final Clean-Up ===\n",
    "train_extended = train_extended.replace([np.inf, -np.inf], np.nan)\n",
    "train_extended = train_extended.dropna(subset=TARGETS, how='all')\n",
    "train_extended = train_extended.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\n📊 Final Summary:\")\n",
    "print(f\"Train: {len(train)} | Extended: {len(train_extended)}\")\n",
    "for t in TARGETS:\n",
    "    base = train[t].notna().sum()\n",
    "    ext = train_extended[t].notna().sum()\n",
    "    print(f\"• {t:<8}: {ext} total ({ext - base:+} from supplements)\")\n",
    "\n",
    "print(\"\\n✅ Data loading and preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071695b6",
   "metadata": {
    "_cell_guid": "82c3bbe7-9513-4cd4-9a9b-e1b0d2ed414b",
    "_uuid": "27cdf121-19be-4d57-bab4-0cf234026932",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:06:00.360464Z",
     "iopub.status.busy": "2025-08-07T09:06:00.360119Z",
     "iopub.status.idle": "2025-08-07T09:06:06.152285Z",
     "shell.execute_reply": "2025-08-07T09:06:06.150765Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.799571,
     "end_time": "2025-08-07T09:06:06.154146",
     "exception": false,
     "start_time": "2025-08-07T09:06:00.354575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 constant columns from train_extended\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374645</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>1.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>c1ccc(-c2ccccn2)nc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>c1ccc(-c2nc3cc4ncoc4cc3o2)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>c1ccc2oc(-c3ccc4ncoc4c3)nc2c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>c1ccsc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>c1csc(-c2cccs2)c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9990 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SMILES  Tg       FFV  \\\n",
       "0                            *CC(*)c1ccccc1C(=O)OCCCCCC NaN  0.374645   \n",
       "1     *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5... NaN  0.370410   \n",
       "2     *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(... NaN  0.378860   \n",
       "3     *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)... NaN  0.387324   \n",
       "4     *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N... NaN  0.355470   \n",
       "...                                                 ...  ..       ...   \n",
       "9985                                c1ccc(-c2ccccn2)nc1 NaN       NaN   \n",
       "9986                      c1ccc(-c2nc3cc4ncoc4cc3o2)cc1 NaN       NaN   \n",
       "9987                      c1ccc2oc(-c3ccc4ncoc4c3)nc2c1 NaN       NaN   \n",
       "9988                                            c1ccsc1 NaN       NaN   \n",
       "9989                                  c1csc(-c2cccs2)c1 NaN       NaN   \n",
       "\n",
       "            Tc  Density  Rg  \n",
       "0     0.205667     1.05 NaN  \n",
       "1          NaN      NaN NaN  \n",
       "2          NaN      NaN NaN  \n",
       "3          NaN      NaN NaN  \n",
       "4          NaN      NaN NaN  \n",
       "...        ...      ...  ..  \n",
       "9985       NaN     1.31 NaN  \n",
       "9986       NaN     1.43 NaN  \n",
       "9987       NaN     1.43 NaN  \n",
       "9988       NaN     1.51 NaN  \n",
       "9989       NaN     1.51 NaN  \n",
       "\n",
       "[9990 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_list = train_extended['SMILES'].tolist()\n",
    "# Clean SMILES column robustly\n",
    "train_extended['SMILES'] = train_extended['SMILES'].apply(clean_and_validate_smiles)\n",
    "# === Final Clean-Up ===\n",
    "train_extended = train_extended.replace([np.inf, -np.inf], np.nan)\n",
    "train_extended = train_extended.dropna(subset=TARGETS, how='all')\n",
    "train_extended = train_extended.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n",
    "\n",
    "# === Drop constant columns ===\n",
    "constant_cols = [col for col in train_extended.columns if train_extended[col].nunique() == 1]\n",
    "train_extended.drop(columns=constant_cols, inplace=True)\n",
    "print(f\"Dropped {len(constant_cols)} constant columns from train_extended\")\n",
    "\n",
    "\n",
    "train_extended.shape\n",
    "train_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49955920",
   "metadata": {
    "_cell_guid": "f805f22f-7ab9-4f41-9189-317aa7410477",
    "_uuid": "34f4d825-34d7-402f-8d1e-3fcfa7cfeaa7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005019,
     "end_time": "2025-08-07T09:06:06.164670",
     "exception": false,
     "start_time": "2025-08-07T09:06:06.159651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064e9b0e",
   "metadata": {
    "_cell_guid": "f07337cc-6d2b-4157-b0f0-65e34d056388",
    "_uuid": "c16d1946-be01-4c05-ad3b-0709f44b6a6e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:06:06.182278Z",
     "iopub.status.busy": "2025-08-07T09:06:06.181921Z",
     "iopub.status.idle": "2025-08-07T09:06:07.381152Z",
     "shell.execute_reply": "2025-08-07T09:06:07.380189Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.207855,
     "end_time": "2025-08-07T09:06:07.382859",
     "exception": false,
     "start_time": "2025-08-07T09:06:06.175004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Canonicalize SMILES ===\n",
    "def canonicalize_smiles(smiles_list):\n",
    "    canonical = []\n",
    "    for smi in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            canonical.append(Chem.MolToSmiles(mol, canonical=True))\n",
    "        else:\n",
    "            canonical.append(None)\n",
    "    return canonical\n",
    "\n",
    "# === All RDKit Descriptors ===\n",
    "def compute_rdkit_descriptors(mol):\n",
    "    descs = {}\n",
    "    for name, func in Descriptors.descList:\n",
    "        try:\n",
    "            descs[name] = func(mol)\n",
    "        except:\n",
    "            descs[name] = np.nan\n",
    "    return descs\n",
    "\n",
    "# === Explicit 5 Key Descriptors ===\n",
    "def compute_key_descriptors(mol):\n",
    "    return {\n",
    "        'key_MolWt': Descriptors.MolWt(mol),\n",
    "        'key_LogP': Descriptors.MolLogP(mol),\n",
    "        'key_RotBonds': Descriptors.NumRotatableBonds(mol),\n",
    "        'key_HDonors': Descriptors.NumHDonors(mol),\n",
    "        'key_HAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "    }\n",
    "\n",
    "# === Graph Features ===\n",
    "def compute_graph_descriptors(mol):\n",
    "    descriptors = {}\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from([(b.GetBeginAtomIdx(), b.GetEndAtomIdx()) for b in mol.GetBonds()])\n",
    "\n",
    "    try:\n",
    "        descriptors['graph_diameter'] = nx.diameter(g) if nx.is_connected(g) else 0\n",
    "        descriptors['avg_shortest_path'] = nx.average_shortest_path_length(g) if nx.is_connected(g) else 0\n",
    "    except:\n",
    "        descriptors['graph_diameter'] = 0\n",
    "        descriptors['avg_shortest_path'] = 0\n",
    "\n",
    "    descriptors['num_cycles'] = len(nx.cycle_basis(g))\n",
    "\n",
    "    try:\n",
    "        descriptors['betweenness_mean'] = np.mean(list(nx.betweenness_centrality(g).values()))\n",
    "        descriptors['betweenness_std'] = np.std(list(nx.betweenness_centrality(g).values()))\n",
    "        descriptors['closeness_mean'] = np.mean(list(nx.closeness_centrality(g).values()))\n",
    "        descriptors['max_degree'] = max(dict(g.degree()).values())\n",
    "    except:\n",
    "        descriptors['betweenness_mean'] = np.nan\n",
    "        descriptors['betweenness_std'] = np.nan\n",
    "        descriptors['closeness_mean'] = np.nan\n",
    "        descriptors['max_degree'] = np.nan\n",
    "\n",
    "    try:\n",
    "        ec = nx.eigenvector_centrality_numpy(g)\n",
    "        descriptors['eigenvector_mean'] = np.mean(list(ec.values()))\n",
    "    except:\n",
    "        descriptors['eigenvector_mean'] = np.nan\n",
    "\n",
    "    try:\n",
    "        ring_info = mol.GetRingInfo().AtomRings()\n",
    "        descriptors['ring_4'] = sum(1 for r in ring_info if len(r) == 4)\n",
    "    except:\n",
    "        descriptors['ring_4'] = 0\n",
    "\n",
    "    try:\n",
    "        descriptors['heteroatom_ratio'] = sum(1 for a in mol.GetAtoms() if a.GetAtomicNum() not in [1, 6]) / mol.GetNumAtoms()\n",
    "    except:\n",
    "        descriptors['heteroatom_ratio'] = np.nan\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "# === Final Combined Feature Computation ===\n",
    "def compute_all_features(smiles_list, verbose=True):\n",
    "    smiles_list = canonicalize_smiles(smiles_list)\n",
    "\n",
    "    feature_dict = {}\n",
    "    valid_idx = []\n",
    "    failed_idx = []\n",
    "\n",
    "    for idx, smi in enumerate(tqdm(smiles_list, desc=\"Computing Features\")):\n",
    "        if smi is None:\n",
    "            failed_idx.append(idx)\n",
    "            continue\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            failed_idx.append(idx)\n",
    "            continue\n",
    "\n",
    "        valid_idx.append(idx)\n",
    "        feats = {}\n",
    "        \n",
    "        # Compute all descriptors from RDKit\n",
    "        feats.update(compute_rdkit_descriptors(mol))\n",
    "        # Compute graph descriptors\n",
    "        feats.update(compute_graph_descriptors(mol))\n",
    "        # Add the 5 explicit key descriptors (overwrites if duplicated keys)\n",
    "        feats.update(compute_key_descriptors(mol))\n",
    "\n",
    "        for k, v in feats.items():\n",
    "            if k not in feature_dict:\n",
    "                feature_dict[k] = []\n",
    "            feature_dict[k].append(v)\n",
    "\n",
    "    total = len(smiles_list)\n",
    "    for k in feature_dict:\n",
    "        if len(feature_dict[k]) < total:\n",
    "            feature_dict[k].extend([None] * (total - len(feature_dict[k])))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n--- Feature Engineering Summary ---\")\n",
    "        print(f\"Total SMILES: {total}\")\n",
    "        print(f\"Valid molecules: {len(valid_idx)}\")\n",
    "        print(f\"Invalid molecules: {len(failed_idx)}\")\n",
    "        print(f\"Number of computed features: {len(feature_dict)}\")\n",
    "        sample_key = next(iter(feature_dict))\n",
    "        print(f\"Feature vector length per molecule: {len(feature_dict[sample_key])}\")\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "    return feature_dict, valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d5bc7e",
   "metadata": {
    "_cell_guid": "473819d0-2955-4fd2-9f11-5f1237d210a4",
    "_uuid": "8ba4c030-2743-4333-a39a-9033d5bd99ab",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:06:07.394412Z",
     "iopub.status.busy": "2025-08-07T09:06:07.393400Z",
     "iopub.status.idle": "2025-08-07T09:12:01.657730Z",
     "shell.execute_reply": "2025-08-07T09:12:01.655924Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 354.272097,
     "end_time": "2025-08-07T09:12:01.659699",
     "exception": false,
     "start_time": "2025-08-07T09:06:07.387602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Features: 100%|██████████| 9990/9990 [05:47<00:00, 28.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering Summary ---\n",
      "Total SMILES: 9990\n",
      "Valid molecules: 9990\n",
      "Invalid molecules: 0\n",
      "Number of computed features: 232\n",
      "Feature vector length per molecule: 9990\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Features: 100%|██████████| 3/3 [00:00<00:00, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering Summary ---\n",
      "Total SMILES: 3\n",
      "Valid molecules: 3\n",
      "Invalid molecules: 0\n",
      "Number of computed features: 232\n",
      "Feature vector length per molecule: 3\n",
      "-----------------------------------\n",
      "Train features shape: (9990, 163)\n",
      "Test features shape: (3, 163)\n",
      "Training dataframe Shape: (9990, 6)\n",
      "Test dataframe Shape: (3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rdkit import RDLogger\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# List of columns to drop ,Source: from various Notebooks through out the competition\n",
    "useless_cols = [   \n",
    "    'MaxPartialCharge', \n",
    "    'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',\n",
    "    'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW',\n",
    "    'NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA9', 'fr_barbitur',\n",
    "    'fr_benzodiazepine', 'fr_dihydropyridine', 'fr_epoxide', 'fr_isothiocyan',\n",
    "    'fr_lactam', 'fr_nitroso', 'fr_prisulfonamd', 'fr_thiocyan',\n",
    "    'MaxEStateIndex', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons',\n",
    "    'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Kappa1',\n",
    "    'LabuteASA', 'HeavyAtomCount', 'MolMR', 'Chi3n', 'BertzCT', 'Chi2v',\n",
    "    'Chi4n', 'HallKierAlpha', 'Chi3v', 'Chi4v', 'MinAbsPartialCharge',\n",
    "    'MinPartialCharge', 'MaxAbsPartialCharge', 'FpDensityMorgan2',\n",
    "    'FpDensityMorgan3', 'Phi', 'Kappa3', 'fr_nitrile', 'SlogP_VSA6',\n",
    "    'NumAromaticCarbocycles', 'NumAromaticRings', 'fr_benzene', 'VSA_EState6',\n",
    "    'NOCount', 'fr_C_O', 'fr_C_O_noCOO', 'NumHDonors', 'fr_amide',\n",
    "    'fr_Nhpyrrole', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_COO2',\n",
    "    'fr_halogen', 'fr_diazo', 'fr_nitro_arom', 'fr_phos_ester'\n",
    "]\n",
    "\n",
    "# === Compute Train Features ===\n",
    "feature_dict_train, valid_idx_train = compute_all_features(train_extended[\"SMILES\"], verbose=True)\n",
    "features_train = pd.DataFrame(feature_dict_train).reset_index(drop=True)\n",
    "features_train = features_train.drop(columns=[col for col in useless_cols if col in features_train.columns])\n",
    "\n",
    "# === Compute Test Features ===\n",
    "feature_dict_test, valid_idx_test = compute_all_features(test[\"SMILES\"], verbose=True)\n",
    "features_test = pd.DataFrame(feature_dict_test).reset_index(drop=True)\n",
    "features_test = features_test.drop(columns=[col for col in useless_cols if col in features_test.columns])\n",
    "\n",
    "# === Output Summary ===\n",
    "print(\"Train features shape:\", features_train.shape)\n",
    "print(\"Test features shape:\", features_test.shape)\n",
    "print(\"Training dataframe Shape:\", train_extended.shape)\n",
    "print(\"Test dataframe Shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b112c7b",
   "metadata": {
    "_cell_guid": "8531cdd0-8740-471c-988e-6b4dffc394ba",
    "_uuid": "8598d056-e3d0-47a2-bf2e-17a94252a91f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.121635,
     "end_time": "2025-08-07T09:12:01.902424",
     "exception": false,
     "start_time": "2025-08-07T09:12:01.780789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerprocessing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b679bd1d",
   "metadata": {
    "_cell_guid": "dcf9580f-ff76-42d4-b8dc-9541c48c9f01",
    "_uuid": "d4c6e0ce-0256-4452-85f0-6e7211906c3c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:12:02.147087Z",
     "iopub.status.busy": "2025-08-07T09:12:02.145712Z",
     "iopub.status.idle": "2025-08-07T09:12:04.193319Z",
     "shell.execute_reply": "2025-08-07T09:12:04.191749Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.17184,
     "end_time": "2025-08-07T09:12:04.195108",
     "exception": false,
     "start_time": "2025-08-07T09:12:02.023268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Preprocessing Train features\n",
      "  - Replacing 0 ±inf values with NaN...\n",
      "  - Dropping 0 all-NaN columns...\n",
      "  - Filling 23 remaining NaNs with column means...\n",
      "\n",
      "📦 Preprocessing Test features\n",
      "  - Replacing 0 ±inf values with NaN...\n",
      "  - Dropping 0 all-NaN columns...\n",
      "  - Filling 0 remaining NaNs with column means...\n",
      "\n",
      "🧹 Applying VarianceThreshold (threshold=1e-05)...\n",
      "  - Removed 0 low-variance features.\n",
      "\n",
      "🧯 Clipping outliers to range [-1000000.0, 1000000.0]...\n",
      "  - Clipping 1 overly large and 0 overly small features.\n",
      "\n",
      "🧯 Clipping outliers to range [-1000000.0, 1000000.0]...\n",
      "  - Clipping 1 overly large and 0 overly small features.\n",
      "\n",
      "✅ Final Preprocessing Summary:\n",
      "  - Train shape: (9990, 163)\n",
      "  - Test shape:  (3, 163)\n",
      "  - Common features retained: 163\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# === Preprocessing Utilities ===\n",
    "\n",
    "def preprocess_features(df, df_name=\"\"):\n",
    "    print(f\"\\n📦 Preprocessing {df_name} features\")\n",
    "\n",
    "    # 1. Replace inf/-inf with NaN\n",
    "    inf_count = np.isinf(df.values).sum()\n",
    "    print(f\"  - Replacing {inf_count} ±inf values with NaN...\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 2. Drop columns that are entirely NaN\n",
    "    all_nan_cols = df.columns[df.isna().all()].tolist()\n",
    "    print(f\"  - Dropping {len(all_nan_cols)} all-NaN columns...\")\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # 3. Fill remaining NaNs with column means\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    print(f\"  - Filling {nan_count} remaining NaNs with column means...\")\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_outliers(df, threshold=1e10):\n",
    "    max_vals = df.max()\n",
    "    min_vals = df.min()\n",
    "    too_large = max_vals[max_vals > threshold]\n",
    "    too_small = min_vals[min_vals < -threshold]\n",
    "    return too_large, too_small\n",
    "\n",
    "\n",
    "def remove_low_variance(df, threshold=1e-5):\n",
    "    print(f\"\\n🧹 Applying VarianceThreshold (threshold={threshold})...\")\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    reduced = selector.fit_transform(df)\n",
    "    kept_cols = df.columns[selector.get_support()]\n",
    "    removed_count = df.shape[1] - len(kept_cols)\n",
    "    print(f\"  - Removed {removed_count} low-variance features.\")\n",
    "    return pd.DataFrame(reduced, columns=kept_cols)\n",
    "\n",
    "\n",
    "def clip_outliers(df, lower=-1e6, upper=1e6):\n",
    "    print(f\"\\n🧯 Clipping outliers to range [{lower}, {upper}]...\")\n",
    "    too_large, too_small = detect_outliers(df)\n",
    "    if not too_large.empty or not too_small.empty:\n",
    "        print(f\"  - Clipping {len(too_large)} overly large and {len(too_small)} overly small features.\")\n",
    "        df = df.clip(lower, upper)\n",
    "    else:\n",
    "        print(\"  - No extreme outliers found.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Apply Preprocessing ===\n",
    "\n",
    "# Make sure your features_train and features_test already exist\n",
    "features_train_clean = preprocess_features(features_train, df_name=\"Train\")\n",
    "features_test_clean = preprocess_features(features_test, df_name=\"Test\")\n",
    "\n",
    "# Align both datasets\n",
    "common_cols = features_train_clean.columns.intersection(features_test_clean.columns)\n",
    "features_train_clean = features_train_clean[common_cols].copy()\n",
    "features_test_clean = features_test_clean[common_cols].copy()\n",
    "\n",
    "# Remove near-zero variance features\n",
    "features_train_clean = remove_low_variance(features_train_clean)\n",
    "features_test_clean = features_test_clean[features_train_clean.columns]  # Align\n",
    "\n",
    "# Clip extreme outliers\n",
    "features_train_clean = clip_outliers(features_train_clean)\n",
    "features_test_clean = clip_outliers(features_test_clean)\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\n✅ Final Preprocessing Summary:\")\n",
    "print(f\"  - Train shape: {features_train_clean.shape}\")\n",
    "print(f\"  - Test shape:  {features_test_clean.shape}\")\n",
    "print(f\"  - Common features retained: {len(features_train_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a81af",
   "metadata": {
    "_cell_guid": "2af3f64c-0855-4a19-bea2-f8cc6c2f54a7",
    "_uuid": "84d7b0d8-2c2b-4554-99ce-a890153e87a1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.120431,
     "end_time": "2025-08-07T09:12:04.436401",
     "exception": false,
     "start_time": "2025-08-07T09:12:04.315970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ExtraTrees Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4136d4",
   "metadata": {
    "_cell_guid": "66700e04-c4cb-4d88-b4ff-8efd60f7b4bb",
    "_uuid": "03909ba7-74da-4be0-81d4-229b7b1062a9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:12:04.686805Z",
     "iopub.status.busy": "2025-08-07T09:12:04.686155Z",
     "iopub.status.idle": "2025-08-07T09:14:39.303949Z",
     "shell.execute_reply": "2025-08-07T09:14:39.302986Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 154.744891,
     "end_time": "2025-08-07T09:14:39.305376",
     "exception": false,
     "start_time": "2025-08-07T09:12:04.560485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training Round 1\n",
      "\n",
      "🎯 Target: Tg\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 39.99535\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 37.04403\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 36.91343\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 35.19417\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 33.59358\n",
      "\n",
      "🎯 Target: FFV\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.00610\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.00638\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.00650\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.00632\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.00651\n",
      "\n",
      "🎯 Target: Tc\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.02951\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.02730\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.04174\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.02415\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.02816\n",
      "\n",
      "🎯 Target: Density\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.05168\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.05013\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.05455\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.04935\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.04945\n",
      "\n",
      "🎯 Target: Rg\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 1.98123\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 1.83445\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 1.96693\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 1.64925\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 1.72080\n",
      "\n",
      "🚀 Training Round 2\n",
      "\n",
      "🎯 Target: Tg\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 39.99535\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 37.04403\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 36.91343\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 35.19417\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 33.59358\n",
      "\n",
      "🎯 Target: FFV\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.00610\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.00638\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.00650\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.00632\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.00651\n",
      "\n",
      "🎯 Target: Tc\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.02951\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.02730\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.04174\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.02415\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.02816\n",
      "\n",
      "🎯 Target: Density\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 0.05168\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 0.05013\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 0.05455\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 0.04935\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 0.04945\n",
      "\n",
      "🎯 Target: Rg\n",
      "  🧪 Fold 1/5\n",
      "     🔍 Fold MAE: 1.98123\n",
      "  🧪 Fold 2/5\n",
      "     🔍 Fold MAE: 1.83445\n",
      "  🧪 Fold 3/5\n",
      "     🔍 Fold MAE: 1.96693\n",
      "  🧪 Fold 4/5\n",
      "     🔍 Fold MAE: 1.64925\n",
      "  🧪 Fold 5/5\n",
      "     🔍 Fold MAE: 1.72080\n",
      "\n",
      "📁 Submission saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# === Configuration ===\n",
    "target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "n_splits = 5\n",
    "random_seed = 42\n",
    "\n",
    "# === Prepare DataFrames to hold predictions ===\n",
    "oof_preds_1 = pd.DataFrame(index=features_train_clean.index, columns=target_cols)\n",
    "test_preds_1 = pd.DataFrame(index=features_test_clean.index, columns=target_cols)\n",
    "\n",
    "oof_preds_2 = pd.DataFrame(index=features_train_clean.index, columns=target_cols)\n",
    "test_preds_2 = pd.DataFrame(index=features_test_clean.index, columns=target_cols)\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "\n",
    "# === Train models and predict twice for ensembling ===\n",
    "for round_num, (oof_df, test_df) in enumerate([(oof_preds_1, test_preds_1), (oof_preds_2, test_preds_2)], start=1):\n",
    "    print(f\"\\n🚀 Training Round {round_num}\")\n",
    "\n",
    "    for target in target_cols:\n",
    "        print(f\"\\n🎯 Target: {target}\")\n",
    "\n",
    "        mask = ~train_extended[target].isna()\n",
    "        X = features_train_clean.loc[mask].copy()\n",
    "        y = train_extended.loc[mask, target].copy()\n",
    "\n",
    "        # Replace inf/-inf with NaN, then impute with mean\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        for col in X.columns:\n",
    "            if X[col].isnull().any():\n",
    "                mean_val = X[col].mean()\n",
    "                X[col].fillna(mean_val, inplace=True)\n",
    "                features_test_clean[col].fillna(mean_val, inplace=True)\n",
    "\n",
    "        oof_pred = np.zeros(len(X))\n",
    "        test_fold_preds = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            print(f\"  🧪 Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = lgb.LGBMRegressor(\n",
    "                objective=\"mae\",\n",
    "                metric=\"mae\",\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.05,\n",
    "                feature_fraction=0.8,\n",
    "                bagging_fraction=0.8,\n",
    "                bagging_freq=1,\n",
    "                lambda_l1=0.1,\n",
    "                lambda_l2=0.1,\n",
    "                num_leaves=31,\n",
    "                verbose=-1,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed + fold\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"mae\",\n",
    "                callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "            )\n",
    "\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(features_test_clean)\n",
    "\n",
    "            oof_pred[val_idx] = val_pred\n",
    "            test_fold_preds.append(test_pred)\n",
    "\n",
    "            fold_mae = mean_absolute_error(y_val, val_pred)\n",
    "            print(f\"     🔍 Fold MAE: {fold_mae:.5f}\")\n",
    "\n",
    "        oof_df.loc[mask, target] = oof_pred\n",
    "        test_df[target] = np.mean(test_fold_preds, axis=0)\n",
    "\n",
    "# === Final submission: average test predictions ===\n",
    "test_preds_avg = (test_preds_1 + test_preds_2) / 2\n",
    "submission = test[['id']].copy()\n",
    "submission = pd.concat([submission, test_preds_avg], axis=1)\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\n📁 Submission saved as submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cddaa3",
   "metadata": {
    "_cell_guid": "c6c35f3d-2617-42c9-8004-39aaceed8d9d",
    "_uuid": "62d8b0a7-5a33-4be1-9a46-75681155af1e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-07T09:14:39.550440Z",
     "iopub.status.busy": "2025-08-07T09:14:39.550117Z",
     "iopub.status.idle": "2025-08-07T09:14:39.561411Z",
     "shell.execute_reply": "2025-08-07T09:14:39.560537Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.135061,
     "end_time": "2025-08-07T09:14:39.563060",
     "exception": false,
     "start_time": "2025-08-07T09:14:39.427999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>136.500149</td>\n",
       "      <td>0.371902</td>\n",
       "      <td>0.189768</td>\n",
       "      <td>1.206412</td>\n",
       "      <td>20.823329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>148.729134</td>\n",
       "      <td>0.375412</td>\n",
       "      <td>0.234676</td>\n",
       "      <td>1.114005</td>\n",
       "      <td>20.398437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>107.948971</td>\n",
       "      <td>0.350409</td>\n",
       "      <td>0.249736</td>\n",
       "      <td>1.119443</td>\n",
       "      <td>20.704481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          Tg       FFV        Tc   Density         Rg\n",
       "0  1109053969  136.500149  0.371902  0.189768  1.206412  20.823329\n",
       "1  1422188626  148.729134  0.375412  0.234676  1.114005  20.398437\n",
       "2  2032016830  107.948971  0.350409  0.249736  1.119443  20.704481"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709500,
     "sourceId": 12235747,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709869,
     "sourceId": 12330396,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 546.694363,
   "end_time": "2025-08-07T09:14:42.083619",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-07T09:05:35.389256",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
